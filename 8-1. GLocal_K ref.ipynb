{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27288d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "from scipy.sparse import csc_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "62086979",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.11.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2098a9b2",
   "metadata": {},
   "source": [
    "# 1. Data Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d649a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_train):\n",
    "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99197ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 20000\n"
     ]
    }
   ],
   "source": [
    "data_path = 'datas/'\n",
    "path = data_path + '/MovieLens_100K/'\n",
    "##\n",
    "# 1. n_m : number of movies\n",
    "# 2. n_u : number of users\n",
    "# 3. train_r : rating datas (train_datas)\n",
    "# 4 . train_m : movies datas (train_datas)\n",
    "# 5,6 . test_r, test_m\n",
    "##\n",
    "\n",
    "n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a78c1b60",
   "metadata": {},
   "source": [
    "# 2. Setting Value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "194ae245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500\n",
    "n_dim = 5\n",
    "n_layers = 2\n",
    "gk_size = 3\n",
    "\n",
    "lambda_2 = 20.  # l2 regularisation\n",
    "lambda_s = 0.006\n",
    "iter_p = 5  # optimisation\n",
    "iter_f = 5\n",
    "epoch_p = 30  # training epoch\n",
    "epoch_f = 60\n",
    "dot_scale = 1  # scaled dot product"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "668f7cb4",
   "metadata": {},
   "source": [
    "# 3. Pre-training with Local Kernel, Item-based AutoEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "406bdd91",
   "metadata": {},
   "source": [
    "## 3-1. Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7a07c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_kernel(u, v):\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "\n",
    "    return hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9763ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.494037, shape=(), dtype=float32) tf.Tensor(13085.154, shape=(), dtype=float32) tf.Tensor(13089.648, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.  0. -0. ...  0.  0.  0.]\n",
      " [ 0.  0.  0. ...  0. -0.  0.]\n",
      " [ 0. -0.  0. ... -0.  0. -0.]\n",
      " ...\n",
      " [-0. -0. -0. ...  0. -0. -0.]\n",
      " [-0.  0.  0. ...  0. -0. -0.]\n",
      " [ 0. -0. -0. ... -0.  0. -0.]], shape=(943, 500), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.51634717 0.50545365 0.5000723  ... 0.5379147  0.5053494  0.5102376 ]\n",
      " [0.51634717 0.50545365 0.5000723  ... 0.5379147  0.5053494  0.5102376 ]\n",
      " [0.51634717 0.50545365 0.5000723  ... 0.5379147  0.5053494  0.5102376 ]\n",
      " ...\n",
      " [0.51634717 0.50545365 0.5000723  ... 0.5379147  0.5053494  0.5102376 ]\n",
      " [0.51634717 0.50545365 0.5000723  ... 0.5379147  0.5053494  0.5102376 ]\n",
      " [0.51634717 0.50545365 0.5000723  ... 0.5379147  0.5053494  0.5102376 ]], shape=(1682, 500), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-09 10:43:06.234545: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/formegusto/opt/anaconda3/lib/python3.8/site-packages/keras/initializers/initializers_v2.py:120: UserWarning: The initializer GlorotUniform is unseeded and being called multiple times, which will return identical values  each time (even if the initializer is unseeded). Please update your code to provide a seed to the initializer, or avoid using the same initalizer instance more than once.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# 1. Pre Training with Local Kernel, Item-Based Auto Encoder\n",
    "n_in = n_u\n",
    "n_dim = 5\n",
    "n_hid = 500\n",
    "lambda_2 = 20.  # l2 regularisation\n",
    "lambda_s = 0.006\n",
    "\n",
    "y = tf.Variable(tf.ones(shape=[n_m, n_u], dtype=float))\n",
    "\n",
    "default_initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W = tf.Variable(default_initializer(shape=[n_in, n_hid]), name='W')\n",
    "u = tf.Variable(tf.random.truncated_normal([n_in, 1, n_dim]), name=\"u\")\n",
    "v = tf.Variable(tf.random.truncated_normal([1, n_hid, n_dim]), name=\"v\")\n",
    "b = tf.Variable(default_initializer(shape=[n_hid]))\n",
    "\n",
    "w_hat = local_kernel(u, v)\n",
    "\n",
    "sparse_reg = tf.keras.regularizers.L2(lambda_s)\n",
    "sparse_reg_term = sparse_reg(w_hat)\n",
    "\n",
    "l2_reg = tf.keras.regularizers.L2(lambda_2)\n",
    "l2_reg_term = l2_reg(W)\n",
    "\n",
    "loss_value = sparse_reg_term + l2_reg_term\n",
    "print(sparse_reg_term, l2_reg_term, loss_value)\n",
    "\n",
    "W_eff = W * w_hat\n",
    "print(W_eff)\n",
    "\n",
    "y = tf.matmul(y, W_eff) + b\n",
    "y = tf.nn.sigmoid(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bef909",
   "metadata": {},
   "source": [
    "## 3-2. Modularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "55383eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500\n",
    "n_dim = 5\n",
    "n_layers = 2\n",
    "gk_size = 3\n",
    "\n",
    "lambda_2 = 20.  # l2 regularisation\n",
    "lambda_s = 0.006\n",
    "iter_p = 5  # optimisation\n",
    "iter_f = 5\n",
    "epoch_p = 30  # training epoch\n",
    "epoch_f = 60\n",
    "dot_scale = 1  # scaled dot product\n",
    "\n",
    "def local_kernel(u, v):\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "\n",
    "    return hat\n",
    "\n",
    "class kernel_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_hid=n_hid, n_dim=n_dim, activation=tf.nn.sigmoid, lambda_s=lambda_s, lambda_2 = lambda_2):\n",
    "        super(kernel_layer, self).__init__()\n",
    "        \n",
    "        self.n_dim = n_dim\n",
    "        self.n_hid = n_hid\n",
    "        self.lambda_2 = lambda_2\n",
    "        self.lambda_s = lambda_s\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        default_initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "        W = tf.Variable(default_initializer(shape=[inputs.shape[1], self.n_hid]), name='W')\n",
    "        n_in = inputs.shape[1]\n",
    "        u = tf.Variable(tf.random.truncated_normal([n_in, 1, self.n_dim]), name=\"u\")\n",
    "        v = tf.Variable(tf.random.truncated_normal([1, self.n_hid, self.n_dim]), name=\"v\")\n",
    "        b = tf.Variable(default_initializer(shape=[self.n_hid]))\n",
    "        \n",
    "        w_hat = local_kernel(u, v)\n",
    "\n",
    "        sparse_reg = tf.keras.regularizers.L2(self.lambda_s)\n",
    "        sparse_reg_term = sparse_reg(w_hat)\n",
    "\n",
    "        l2_reg = tf.keras.regularizers.L2(self.lambda_2)\n",
    "        l2_reg_term = l2_reg(W)\n",
    "\n",
    "        loss_value = sparse_reg_term + l2_reg_term\n",
    "\n",
    "        W_eff = W * w_hat\n",
    "\n",
    "        y = tf.matmul(inputs, W_eff) + b\n",
    "        y = self.activation(y)\n",
    "\n",
    "        return y, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "31795fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "y = tf.Variable(tf.ones(shape=[n_m, n_u], dtype=float))\n",
    "reg_losses = None\n",
    "\n",
    "k_p = kernel_layer()\n",
    "\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = k_p(y)\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "    \n",
    "k_p2 = kernel_layer(n_u, activation=tf.identity)\n",
    "pred_p, reg_loss = k_p2(y)\n",
    "reg_losses += reg_loss\n",
    "\n",
    "# L2 Loss\n",
    "diff = train_m * (train_r - pred_p)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss_p = sqE + reg_losses\n",
    "\n",
    "optimizer_p = tf.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a72883d",
   "metadata": {},
   "source": [
    "# 3. Fine Tunning with Global Kernel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "a97e67b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_kernel(inputs, gk_size, dot_scale):\n",
    "    avg_pooling = tf.reduce_mean(inputs, axis=1)  # Item (axis=1) based average pooling\n",
    "    avg_pooling = tf.reshape(avg_pooling, [1, -1])\n",
    "    n_kernel = avg_pooling.shape[1]\n",
    "\n",
    "    conv_kernel = tf.Variable(tf.random.truncated_normal([n_kernel, gk_size**2], stddev=0.1),name=\"conv_kernel\")\n",
    "    gk = tf.matmul(avg_pooling, conv_kernel) * dot_scale  # Scaled dot product\n",
    "    gk = tf.reshape(gk, [gk_size, gk_size, 1, 1])\n",
    "\n",
    "    return gk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "cefad409",
   "metadata": {},
   "outputs": [],
   "source": [
    "def global_conv(inputs, W):\n",
    "    inputs = tf.reshape(inputs, [1, inputs.shape[0], inputs.shape[1], 1])\n",
    "    conv2d = tf.nn.relu(tf.nn.conv2d(inputs, W, strides=[1,1,1,1], padding='SAME'))\n",
    "\n",
    "    return tf.reshape(conv2d, [conv2d.shape[1], conv2d.shape[2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "37020e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.Variable(tf.ones(shape=[n_m, n_u], dtype=float))\n",
    "reg_losses = None\n",
    "\n",
    "k_f = kernel_layer()\n",
    "\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = k_f(y)\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "    \n",
    "k_f2 = kernel_layer(n_u, activation=tf.identity)\n",
    "y_dash, _ = k_f2(y)\n",
    "\n",
    "gk = global_kernel(y_dash, gk_size, dot_scale)\n",
    "y_hat = global_conv(train_r, gk)\n",
    "\n",
    "k_f3 = kernel_layer()\n",
    "for i in range(n_layers):\n",
    "    y_hat, reg_losses = k_f3(y_hat)\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "\n",
    "k_f4 = kernel_layer(n_u, activation=tf.identity)\n",
    "pred_f, reg_loss = k_f4(y_hat)\n",
    "reg_losses += reg_loss\n",
    "\n",
    "# L2 loss\n",
    "diff = train_m * (train_r - pred_f)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss_f = sqE + reg_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41cebcfa",
   "metadata": {},
   "source": [
    "# 4. Evaluation Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "1643a3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dcg_k(score_label, k):\n",
    "    dcg, i = 0., 0\n",
    "    for s in score_label:\n",
    "        if i < k:\n",
    "            dcg += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    return dcg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "01da6974",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ndcg_k(y_hat, y, k):\n",
    "    score_label = np.stack([y_hat, y], axis=1).tolist()\n",
    "    score_label = sorted(score_label, key=lambda d:d[0], reverse=True)\n",
    "    score_label_ = sorted(score_label, key=lambda d:d[1], reverse=True)\n",
    "    norm, i = 0., 0\n",
    "    for s in score_label_:\n",
    "        if i < k:\n",
    "            norm += (2**s[1]-1) / np.log2(2+i)\n",
    "            i += 1\n",
    "    dcg = dcg_k(score_label, k)\n",
    "    return dcg / norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "d94b5f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_ndcg(y_hat, y):\n",
    "    ndcg_sum, num = 0, 0\n",
    "    y_hat, y = y_hat.T, y.T\n",
    "    n_users = y.shape[0]\n",
    "\n",
    "    for i in range(n_users):\n",
    "        y_hat_i = y_hat[i][np.where(y[i])]\n",
    "        y_i = y[i][np.where(y[i])]\n",
    "\n",
    "        if y_i.shape[0] < 2:\n",
    "            continue\n",
    "\n",
    "        ndcg_sum += ndcg_k(y_hat_i, y_i, y_i.shape[0])  # user-wise calculation\n",
    "        num += 1\n",
    "\n",
    "    return ndcg_sum / num"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491e886d",
   "metadata": {},
   "source": [
    "# 5. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9fe6c90b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "class GLocalK(keras.Model):\n",
    "    def __init__(self, **kwargs):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "c2dc2d9c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "local kernel loss : 18902.314453125\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18888.63671875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18885.46484375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18895.3359375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18874.32421875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18838.123046875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18860.828125\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18871.98828125\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18865.12109375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18849.162109375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18855.626953125\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18860.19921875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18887.4609375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18874.08984375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18882.181640625\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18844.484375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18882.44140625\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18857.015625\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18859.03515625\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18841.818359375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18861.6875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18857.060546875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18874.38671875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18887.626953125\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18893.75\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18870.216796875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18865.18359375\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18851.404296875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18849.3671875\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "local kernel loss : 18875.119140625\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
      "PRE-TRAINING\n",
      "Epoch: 2 test rmse: 2.7859828 train rmse: 2.7647288\n",
      ".-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(epoch_p):\n",
    "    with tf.GradientTape() as tape:\n",
    "        output, loss = k_p2(train_r)\n",
    "        print(\"local kernel loss : {}\".format(loss))\n",
    "    gradients = tape.gradient(loss, k_p2.trainable_variables, unconnected_gradients=tf.UnconnectedGradients.ZERO)\n",
    "    optimizer_p.apply_gradients(zip(gradients, k_p2.trainable_variables))\n",
    "    \n",
    "    pre, _ = k_p2(train_r)\n",
    "\n",
    "    error = (test_m * (np.clip(pre, 1., 5.) - test_r) ** 2).sum() / test_m.sum()  # test error\n",
    "    test_rmse = np.sqrt(error)\n",
    "\n",
    "    error_train = (train_m * (np.clip(pre, 1., 5.) - train_r) ** 2).sum() / train_m.sum()  # train error\n",
    "    train_rmse = np.sqrt(error_train)\n",
    "\n",
    "    print('.-^-._' * 12)\n",
    "    print('PRE-TRAINING')\n",
    "    print('Epoch:', i+1, 'test rmse:', test_rmse, 'train rmse:', train_rmse)\n",
    "    print('.-^-._' * 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "bc2e3dd8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<tf.Tensor: shape=(1682, 943), dtype=float32, numpy=\n",
       " array([[-0.07430239, -0.04062451,  0.23960903, ...,  0.01779529,\n",
       "         -0.0182954 , -0.02805335],\n",
       "        [ 0.02291551, -0.02921245,  0.05562797, ...,  0.01779529,\n",
       "         -0.03620867, -0.02805335],\n",
       "        [ 0.02991716, -0.03307576,  0.02324167, ...,  0.01779529,\n",
       "         -0.03620867, -0.02805335],\n",
       "        ...,\n",
       "        [ 0.02291551, -0.03307576,  0.01562973, ...,  0.01779529,\n",
       "         -0.03620867, -0.02805335],\n",
       "        [ 0.02291551, -0.03307576,  0.01562973, ...,  0.01779529,\n",
       "         -0.03620867, -0.02805335],\n",
       "        [ 0.02291551, -0.03307576,  0.01562973, ...,  0.01779529,\n",
       "         -0.03620867, -0.02805335]], dtype=float32)>,\n",
       " <tf.Tensor: shape=(), dtype=float32, numpy=18867.727>)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc31466",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9",
   "language": "python",
   "name": "base"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27288d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %tensorflow_version 1.x\n",
    "from time import time\n",
    "from scipy.sparse import csc_matrix\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import h5py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d649a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_100k(path='./', delimiter='\\t'):\n",
    "\n",
    "    train = np.loadtxt(path+'movielens_100k_u1.base', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    test = np.loadtxt(path+'movielens_100k_u1.test', skiprows=0, delimiter=delimiter).astype('int32')\n",
    "    total = np.concatenate((train, test), axis=0)\n",
    "\n",
    "    n_u = np.unique(total[:,0]).size  # num of users\n",
    "    n_m = np.unique(total[:,1]).size  # num of movies\n",
    "    n_train = train.shape[0]  # num of training ratings\n",
    "    n_test = test.shape[0]  # num of test ratings\n",
    "\n",
    "    train_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "    test_r = np.zeros((n_m, n_u), dtype='float32')\n",
    "\n",
    "    for i in range(n_train):\n",
    "        train_r[train[i,1]-1, train[i,0]-1] = train[i,2]\n",
    "\n",
    "    for i in range(n_test):\n",
    "        test_r[test[i,1]-1, test[i,0]-1] = test[i,2]\n",
    "\n",
    "    train_m = np.greater(train_r, 1e-12).astype('float32')  # masks indicating non-zero entries\n",
    "    test_m = np.greater(test_r, 1e-12).astype('float32')\n",
    "\n",
    "    print('data matrix loaded')\n",
    "    print('num of users: {}'.format(n_u))\n",
    "    print('num of movies: {}'.format(n_m))\n",
    "    print('num of training ratings: {}'.format(n_train))\n",
    "    print('num of test ratings: {}'.format(n_test))\n",
    "\n",
    "    return n_m, n_u, train_r, train_m, test_r, test_m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7d65bb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Common hyperparameter settings\n",
    "n_hid = 500\n",
    "n_dim = 5\n",
    "n_layers = 2\n",
    "gk_size = 3\n",
    "\n",
    "lambda_2 = 20.  # l2 regularisation\n",
    "lambda_s = 0.006\n",
    "iter_p = 5  # optimisation\n",
    "iter_f = 5\n",
    "epoch_p = 30  # training epoch\n",
    "epoch_f = 60\n",
    "dot_scale = 1  # scaled dot product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0c0bdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Insert the path of a data directory by yourself (e.g., '/content/.../data')\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._\n",
    "data_path = 'datas/'\n",
    "# .-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._.-^-._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c99197ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data matrix loaded\n",
      "num of users: 943\n",
      "num of movies: 1682\n",
      "num of training ratings: 80000\n",
      "num of test ratings: 20000\n"
     ]
    }
   ],
   "source": [
    "path = data_path + '/MovieLens_100K/'\n",
    "n_m, n_u, train_r, train_m, test_r, test_m = load_data_100k(path=path, delimiter='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a07c025",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_kernel(u, v):\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "\n",
    "    return hat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9763ad59",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(4.4011245, shape=(), dtype=float32) tf.Tensor(13054.93, shape=(), dtype=float32) tf.Tensor(13059.331, shape=(), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[ 0.          0.          0.         ...  0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ... -0.         -0.\n",
      "  -0.        ]\n",
      " [-0.         -0.         -0.         ...  0.          0.\n",
      "  -0.        ]\n",
      " ...\n",
      " [ 0.          0.          0.         ... -0.         -0.\n",
      "   0.02342543]\n",
      " [-0.         -0.          0.         ...  0.         -0.\n",
      "   0.        ]\n",
      " [-0.          0.         -0.         ...  0.          0.\n",
      "   0.        ]], shape=(943, 500), dtype=float32)\n",
      "tf.Tensor(\n",
      "[[0.47870147 0.49157372 0.51399034 ... 0.4814304  0.5237499  0.50520736]\n",
      " [0.47870147 0.49157372 0.51399034 ... 0.4814304  0.5237499  0.50520736]\n",
      " [0.47870147 0.49157372 0.51399034 ... 0.4814304  0.5237499  0.50520736]\n",
      " ...\n",
      " [0.47870147 0.49157372 0.51399034 ... 0.4814304  0.5237499  0.50520736]\n",
      " [0.47870147 0.49157372 0.51399034 ... 0.4814304  0.5237499  0.50520736]\n",
      " [0.47870147 0.49157372 0.51399034 ... 0.4814304  0.5237499  0.50520736]], shape=(1682, 500), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "#AE Model\n",
    "n_in = n_u\n",
    "n_dim = 5\n",
    "n_hid = 500\n",
    "lambda_2 = 20.  # l2 regularisation\n",
    "lambda_s = 0.006\n",
    "\n",
    "# y = tf.compat.v1.placeholder(\"float\", [n_m, n_u])\n",
    "y = tf.Variable(tf.ones(shape=[n_m, n_u], dtype=float))\n",
    "\n",
    "default_initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "W = tf.Variable(default_initializer(shape=[n_in, n_hid]), name='W')\n",
    "u = tf.Variable(tf.random.truncated_normal([n_in, 1, n_dim]), name=\"u\")\n",
    "v = tf.Variable(tf.random.truncated_normal([1, n_hid, n_dim]), name=\"v\")\n",
    "b = tf.Variable(default_initializer(shape=[n_hid]))\n",
    "\n",
    "w_hat = local_kernel(u, v)\n",
    "\n",
    "sparse_reg = tf.keras.regularizers.L2(lambda_s)\n",
    "sparse_reg_term = sparse_reg(w_hat)\n",
    "\n",
    "l2_reg = tf.keras.regularizers.L2(lambda_2)\n",
    "l2_reg_term = l2_reg(W)\n",
    "\n",
    "loss_value = sparse_reg_term + l2_reg_term\n",
    "print(sparse_reg_term, l2_reg_term, loss_value)\n",
    "\n",
    "W_eff = W * w_hat\n",
    "print(W_eff)\n",
    "\n",
    "y = tf.matmul(y, W_eff) + b\n",
    "y = tf.nn.sigmoid(y)\n",
    "\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "55383eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def local_kernel(u, v):\n",
    "    dist = tf.norm(u - v, ord=2, axis=2)\n",
    "    hat = tf.maximum(0., 1. - dist**2)\n",
    "\n",
    "    return hat\n",
    "\n",
    "class kernel_layer(tf.keras.layers.Layer):\n",
    "    def __init__(self, n_hid=500, activation=tf.nn.sigmoid):\n",
    "        super(kernel_layer, self).__init__()\n",
    "        \n",
    "        self.n_dim = 5\n",
    "        self.n_hid = n_hid\n",
    "        self.lambda_2 = 20.  # l2 regularisation\n",
    "        self.lambda_s = 0.006\n",
    "        \n",
    "        self.activation = activation\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        default_initializer = tf.keras.initializers.GlorotUniform()\n",
    "\n",
    "        W = tf.Variable(default_initializer(shape=[inputs.shape[1], self.n_hid]), name='W')\n",
    "        n_in = inputs.shape[1]\n",
    "        u = tf.Variable(tf.random.truncated_normal([n_in, 1, self.n_dim]), name=\"u\")\n",
    "        v = tf.Variable(tf.random.truncated_normal([1, self.n_hid, self.n_dim]), name=\"v\")\n",
    "        b = tf.Variable(default_initializer(shape=[self.n_hid]))\n",
    "        \n",
    "        w_hat = local_kernel(u, v)\n",
    "\n",
    "        sparse_reg = tf.keras.regularizers.L2(self.lambda_s)\n",
    "        sparse_reg_term = sparse_reg(w_hat)\n",
    "\n",
    "        l2_reg = tf.keras.regularizers.L2(self.lambda_2)\n",
    "        l2_reg_term = l2_reg(W)\n",
    "\n",
    "        loss_value = sparse_reg_term + l2_reg_term\n",
    "\n",
    "        W_eff = W * w_hat\n",
    "\n",
    "        y = tf.matmul(inputs, W_eff) + b\n",
    "        y = self.activation(y)\n",
    "\n",
    "        return y, loss_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "31795fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.Variable(tf.ones(shape=[n_m, n_u], dtype=float))\n",
    "reg_losses = None\n",
    "\n",
    "kl = kernel_layer()\n",
    "\n",
    "for i in range(n_layers):\n",
    "    y, reg_loss = kl(y)\n",
    "    reg_losses = reg_loss if reg_losses is None else reg_losses + reg_loss\n",
    "    \n",
    "\n",
    "kl2 = kernel_layer(n_u, activation=tf.identity)\n",
    "pred_p, reg_loss = kl2(y)\n",
    "reg_losses += reg_loss\n",
    "\n",
    "# L2 Loss\n",
    "diff = train_m * (train_r - pred_p)\n",
    "sqE = tf.nn.l2_loss(diff)\n",
    "loss_p = sqE + reg_losses\n",
    "\n",
    "optimizer = tf.optimizers.SGD()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "55af21c0",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_probability'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/52/4yvp7r991px0gmq4wyr894j40000gn/T/ipykernel_48638/184336803.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_probability\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtfp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_probability'"
     ]
    }
   ],
   "source": [
    "import tensorflow_probability as tfp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff90b59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ffbaa531",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "30b3afe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDenseLayer(tf.keras.layers.Layer):\n",
    "  def __init__(self, num_outputs):\n",
    "    super(MyDenseLayer, self).__init__()\n",
    "    self.num_outputs = num_outputs\n",
    "\n",
    "  def build(self, input_shape):\n",
    "    self.kernel = self.add_weight(\"kernel\",\n",
    "                                  shape=[int(input_shape[-1]),\n",
    "                                         self.num_outputs])\n",
    "\n",
    "  def call(self, inputs):\n",
    "    return tf.matmul(inputs, self.kernel)\n",
    "\n",
    "layer = MyDenseLayer(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eae92ab7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
